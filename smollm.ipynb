{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQEpaYz430iT8nihwCKFQm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilnbsingh/vision/blob/main/smollm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y7Rc9_dhXLD",
        "outputId": "a7ae9e37-807d-4a55-81da-6772ff9132db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Installing required Python packages...\n",
            "Installation complete.\n",
            "================================================================================\n",
            "2. Configuring Qualcomm AI Hub...\n",
            "2025-08-10 11:50:47.261 - INFO - Enabling verbose logging.\n",
            "/usr/local/lib/python3.11/dist-packages/qai_hub/_cli.py:384: UserWarning: Overwriting configuration: /root/.qai_hub/client.ini (previous configuration saved to /root/.qai_hub/client.ini.bak)\n",
            "  warnings.warn(\n",
            "qai-hub configuration saved to /root/.qai_hub/client.ini\n",
            "==================== /root/.qai_hub/client.ini ====================\n",
            "[api]\n",
            "api_token = nak7kyh0inngt9vewsxy74gobp4mk6q5zeean82x\n",
            "api_url = https://app.aihub.qualcomm.com\n",
            "web_url = https://app.aihub.qualcomm.com\n",
            "verbose = True\n",
            "\n",
            "\n",
            "Configuration complete. Your API token is now set.\n",
            "================================================================================\n",
            "3. Downloading and preparing model: HuggingFaceTB/SmolLM-135M-Instruct...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporting model to ONNX with a dummy input of shape: torch.Size([1, 7])...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py:82: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model successfully exported to SmolLM-135M-Instruct.onnx\n",
            "================================================================================\n",
            "4. Checking for device availability: QCS8550 (Proxy)...\n",
            "Device 'QCS8550 (Proxy)' is available. Submitting compilation job...\n",
            "Uploading SmolLM-135M-Instruct.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 622M/622M [00:05<00:00, 110MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scheduled compile job (jp294jnxg) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/jp294jnxg/\n",
            "\n",
            "Compilation job submitted.\n",
            "Could not retrieve job ID from the CompileJob object.\n",
            "You can check the job status and details on the AI Hub website using the URL: https://app.aihub.qualcomm.com/jobs/jp294jnxg/\n",
            "Waiting for compilation to complete...\n",
            "Waiting for compile job (jp294jnxg) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ✅ SUCCESS                          \u0007\n",
            "Compilation job completed successfully!\n",
            "Compiled model is ready, but could not retrieve the Model ID from the Model object.\n",
            "Please check the Qualcomm AI Hub website to find the model and its ID.\n",
            "================================================================================\n",
            "5. Submitting inference job...\n",
            "Input prompt: 'What is the capital of Japan?'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Uploading dataset: 17.7kB [00:00, 122kB/s]                    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scheduled inference job (jpyjqn0rp) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/jpyjqn0rp/\n",
            "\n",
            "Inference job submitted.\n",
            "Could not retrieve job ID from the InferenceJob object.\n",
            "You can check the job status and details on the AI Hub website using the URL: https://app.aihub.qualcomm.com/jobs/jpyjqn0rp/\n",
            "Waiting for inference to complete...\n",
            "Waiting for inference job (jpyjqn0rp) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ❌ FAILED               For input 0, expected int32 for data input dtype but got int64.            \u0007\n",
            "Inference job completed successfully!\n",
            "An error occurred during inference: 'InferenceJob' object has no attribute 'get_output_data'\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Google Colab Notebook for Running SmolLM on Qualcomm QCS8550\n",
        "# This script is updated to compile the model for the NPU and perform inference\n",
        "# using the `submit_inference_job` API.\n",
        "#\n",
        "# Prerequisites:\n",
        "# - A valid Qualcomm AI Hub account.\n",
        "# - An API token from your Qualcomm AI Hub account settings.\n",
        "# - The target device 'QCS8550 (Proxy)' is available on the AI Hub.\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Setup the environment\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Install the required Python packages.\n",
        "# 'qai-hub' is for the Qualcomm AI Hub API.\n",
        "# 'qai-hub-models' provides helper utilities.\n",
        "# 'transformers' and 'torch' are for loading the model.\n",
        "# 'onnx' is required for exporting the model to ONNX format.\n",
        "print(\"1. Installing required Python packages...\")\n",
        "#!pip install qai-hub qai-hub-models torch transformers onnx\n",
        "print(\"Installation complete.\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Configure Qualcomm AI Hub Access\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import qai_hub as hub\n",
        "\n",
        "# You must configure your API token to authenticate with the AI Hub.\n",
        "# Replace \"<YOUR_API_TOKEN>\" with your actual token.\n",
        "# DO NOT share your token.\n",
        "print(\"2. Configuring Qualcomm AI Hub...\")\n",
        "api_token = \"nak7kyh0inngt9vewsxy74gobp4mk6q5zeean82x\" # <-- IMPORTANT: Replace with your API token\n",
        "\n",
        "!qai-hub configure --api_token {api_token}\n",
        "print(\"Configuration complete. Your API token is now set.\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Download and Prepare the SmolLM Model\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "\n",
        "# We'll use the SmolLM-135M-Instruct model from Hugging Face as an example.\n",
        "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
        "print(f\"3. Downloading and preparing model: {model_name}...\")\n",
        "\n",
        "# Load the tokenizer and the model from Hugging Face.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode. This is important for conversion.\n",
        "model.eval()\n",
        "\n",
        "# To handle the dynamic nature of the past_key_values cache, we will\n",
        "# create a simple wrapper class for the model's forward pass.\n",
        "# This ensures the output is a simple tuple of tensors, which ONNX can handle.\n",
        "class SmolLMWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = model.config\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # We need to get the output from the model's forward pass.\n",
        "        # We'll set return_dict to True to easily access the outputs.\n",
        "        outputs = self.model(input_ids, return_dict=True)\n",
        "\n",
        "        # The outputs contain a DynamicCache object, which the ONNX exporter\n",
        "        # cannot handle. We need to convert it into a flat list of tensors.\n",
        "        logits = outputs.logits\n",
        "        past_key_values = outputs.past_key_values\n",
        "\n",
        "        # Flatten the tuple of tuples of tensors into a single tuple of tensors.\n",
        "        flattened_past_key_values = []\n",
        "        for layer_key_value in past_key_values:\n",
        "            flattened_past_key_values.extend(layer_key_value)\n",
        "\n",
        "        # The ONNX exporter requires the output to be a tuple of tensors.\n",
        "        return (logits,) + tuple(flattened_past_key_values)\n",
        "\n",
        "# Instantiate the wrapper and get the output names\n",
        "model_wrapper = SmolLMWrapper(model)\n",
        "num_layers = model.config.num_hidden_layers\n",
        "output_names = ['logits'] + [f'past_key_values_{i}' for i in range(num_layers * 2)]\n",
        "\n",
        "# Define dummy input\n",
        "onnx_model_path = \"SmolLM-135M-Instruct.onnx\"\n",
        "# The tokenizer returns int64 tensors by default.\n",
        "dummy_input = tokenizer(\"What is the capital of France?\", return_tensors=\"pt\").input_ids\n",
        "input_shape = dummy_input.shape\n",
        "\n",
        "print(f\"Exporting model to ONNX with a dummy input of shape: {input_shape}...\")\n",
        "\n",
        "try:\n",
        "    # Use the wrapper model for export.\n",
        "    torch.onnx.export(\n",
        "        model_wrapper,\n",
        "        dummy_input,\n",
        "        onnx_model_path,\n",
        "        opset_version=14,  # Choose a compatible ONNX opset version\n",
        "        input_names=['input_ids'],\n",
        "        output_names=output_names,\n",
        "        dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'}},\n",
        "    )\n",
        "    print(f\"Model successfully exported to {onnx_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during ONNX export: {e}\")\n",
        "    onnx_model_path = None\n",
        "print(\"=\"*80)\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Compile the Model for QCS8550 NPU\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Define the target device\n",
        "#target_device = hub.Device(\"QCS8250 (Proxy)\")\n",
        "target_device = hub.Device(\"QCS8550 (Proxy)\")\n",
        "# Check if the device is available before submitting the job\n",
        "print(f\"4. Checking for device availability: {target_device.name}...\")\n",
        "available_devices = hub.get_devices()\n",
        "if target_device.name not in [d.name for d in available_devices]:\n",
        "    print(f\"ERROR: The device '{target_device.name}' is not currently available.\")\n",
        "    print(\"Please check the Qualcomm AI Hub website for device status and try again later.\")\n",
        "    compiled_model = None\n",
        "else:\n",
        "    print(f\"Device '{target_device.name}' is available. Submitting compilation job...\")\n",
        "    # Submit a compilation job to the Qualcomm AI Hub using the ONNX model file.\n",
        "    if onnx_model_path:\n",
        "        try:\n",
        "            # We are now targeting the QNN runtime for NPU execution.\n",
        "            compile_job = hub.submit_compile_job(\n",
        "                model=onnx_model_path,\n",
        "                name=f\"{model_name.split('/')[-1]}_qcs8550_npu\", # Updated name\n",
        "                device=target_device,\n",
        "                # The input specs must specify the correct integer type.\n",
        "                # The compiler infers int64 from the ONNX model, so we must match that.\n",
        "                # The --truncate_64bit_io option handles the conversion for the NPU.\n",
        "                input_specs={\"input_ids\": (input_shape, \"int64\")},\n",
        "                options=\"--truncate_64bit_io\"\n",
        "            )\n",
        "\n",
        "            print(\"Compilation job submitted.\")\n",
        "            try:\n",
        "                print(f\"Job ID: {compile_job.id}\")\n",
        "            except AttributeError:\n",
        "                print(\"Could not retrieve job ID from the CompileJob object.\")\n",
        "                print(f\"You can check the job status and details on the AI Hub website using the URL: {compile_job.url}\")\n",
        "\n",
        "            # Wait for the job to complete. This is a blocking call and will\n",
        "            # raise an exception if the job fails.\n",
        "            print(\"Waiting for compilation to complete...\")\n",
        "            compile_job.wait()\n",
        "\n",
        "            # If wait() completes successfully, the model is ready.\n",
        "            print(\"Compilation job completed successfully!\")\n",
        "            compiled_model = compile_job.get_target_model()\n",
        "\n",
        "            # Handle the case where the Model object might not have an 'id' attribute.\n",
        "            try:\n",
        "                print(f\"Compiled model is ready. Model ID: {compiled_model.id}\")\n",
        "            except AttributeError:\n",
        "                print(\"Compiled model is ready, but could not retrieve the Model ID from the Model object.\")\n",
        "                print(\"Please check the Qualcomm AI Hub website to find the model and its ID.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during compilation: {e}\")\n",
        "            compiled_model = None\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Run Inference on the NPU using the Compiled Model\n",
        "# ------------------------------------------------------------------------------\n",
        "# Now we can submit an inference job to generate a response.\n",
        "if compiled_model:\n",
        "    print(\"5. Submitting inference job...\")\n",
        "    try:\n",
        "        # Define the input prompt.\n",
        "        prompt_text = \"What is the capital of Japan?\"\n",
        "        print(f\"Input prompt: '{prompt_text}'\")\n",
        "\n",
        "        # The inference job expects a dictionary of inputs. We need to tokenize\n",
        "        # the prompt and convert it to a NumPy array.\n",
        "        input_tokens = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
        "        # We pass the input data as int64, as that is what the compiler inferred from the ONNX model.\n",
        "        # The --truncate_64bit_io option will handle the conversion.\n",
        "        input_data = {\"input_ids\": [input_tokens.cpu().numpy()]}\n",
        "\n",
        "        # Submit the inference job with the compiled model and input data.\n",
        "        inference_job = hub.submit_inference_job(\n",
        "            model=compiled_model,\n",
        "            device=target_device,\n",
        "            inputs=input_data\n",
        "        )\n",
        "\n",
        "        print(\"Inference job submitted.\")\n",
        "        try:\n",
        "            print(f\"Job ID: {inference_job.id}\")\n",
        "        except AttributeError:\n",
        "            print(\"Could not retrieve job ID from the InferenceJob object.\")\n",
        "            print(f\"You can check the job status and details on the AI Hub website using the URL: {inference_job.url}\")\n",
        "\n",
        "        # Wait for the inference job to complete.\n",
        "        print(\"Waiting for inference to complete...\")\n",
        "        inference_job.wait()\n",
        "\n",
        "        # If wait() completes successfully, the output is ready.\n",
        "        print(\"Inference job completed successfully!\")\n",
        "\n",
        "        # Get the output data, which will be a dictionary of numpy arrays.\n",
        "        # We've fixed the method name from get_outputs() to get_output_data().\n",
        "        output_data = inference_job.get_output_data()\n",
        "\n",
        "        # We can then process the output logits to get the generated text.\n",
        "        # This part requires a custom generation loop, as the AI Hub returns\n",
        "        # a single step of output. For a complete LLM inference, you would\n",
        "        # loop this process. Here, we'll just show the raw output as an example.\n",
        "        if 'logits' in output_data:\n",
        "            logits = output_data['logits']\n",
        "            # Here you would typically process the logits to generate the next token.\n",
        "            # For demonstration, we'll just print a confirmation.\n",
        "            print(f\"Received logits with shape: {logits.shape}\")\n",
        "            print(\"The model has successfully processed the input on the NPU.\")\n",
        "            print(\"To generate a full response, you would need to implement a token generation loop.\")\n",
        "        else:\n",
        "            print(\"Output data does not contain 'logits'.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during inference: {e}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# 3. Download and Prepare the SmolLM Model\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "\n",
        "# We'll use the SmolLM-135M-Instruct model from Hugging Face as an example.\n",
        "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
        "print(f\"3. Downloading and preparing model: {model_name}...\")\n",
        "\n",
        "# Load the tokenizer and the model from Hugging Face.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode. This is important for conversion.\n",
        "model.eval()\n",
        "\n",
        "# To handle the dynamic nature of the past_key_values cache, we will\n",
        "# create a simple wrapper class for the model's forward pass.\n",
        "# This ensures the output is a simple tuple of tensors, which ONNX can handle.\n",
        "class SmolLMWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = model.config\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # We need to get the output from the model's forward pass.\n",
        "        # We'll set return_dict to True to easily access the outputs.\n",
        "        outputs = self.model(input_ids, return_dict=True)\n",
        "\n",
        "        # The outputs contain a DynamicCache object, which the ONNX exporter\n",
        "        # cannot handle. We need to convert it into a flat list of tensors.\n",
        "        logits = outputs.logits\n",
        "        past_key_values = outputs.past_key_values\n",
        "\n",
        "        # Flatten the tuple of tuples of tensors into a single tuple of tensors.\n",
        "        flattened_past_key_values = []\n",
        "        for layer_key_value in past_key_values:\n",
        "            flattened_past_key_values.extend(layer_key_value)\n",
        "\n",
        "        # The ONNX exporter requires the output to be a tuple of tensors.\n",
        "        return (logits,) + tuple(flattened_past_key_values)\n",
        "\n",
        "# Instantiate the wrapper and get the output names\n",
        "model_wrapper = SmolLMWrapper(model)\n",
        "num_layers = model.config.num_hidden_layers\n",
        "output_names = ['logits'] + [f'past_key_values_{i}' for i in range(num_layers * 2)]\n",
        "\n",
        "# Define dummy input\n",
        "onnx_model_path = \"SmolLM-135M-Instruct.onnx\"\n",
        "# The tokenizer returns int64 tensors by default. We explicitly cast the dummy input to int32\n",
        "# to create an ONNX model that natively expects int32.\n",
        "dummy_input = tokenizer(\"What is the capital of France?\", return_tensors=\"pt\").input_ids.to(torch.int32)\n",
        "input_shape = dummy_input.shape\n",
        "\n",
        "print(f\"Exporting model to ONNX with a dummy input of shape: {input_shape}...\")\n",
        "\n",
        "try:\n",
        "    # Use the wrapper model for export.\n",
        "    torch.onnx.export(\n",
        "        model_wrapper,\n",
        "        dummy_input,\n",
        "        onnx_model_path,\n",
        "        opset_version=14,  # Choose a compatible ONNX opset version\n",
        "        input_names=['input_ids'],\n",
        "        output_names=output_names,\n",
        "        dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'}},\n",
        "    )\n",
        "    print(f\"Model successfully exported to {onnx_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during ONNX export: {e}\")\n",
        "    onnx_model_path = None\n",
        "print(\"=\"*80)\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Compile the Model for QCS8550 NPU\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Define the target device\n",
        "#target_device = hub.Device(\"QCS8250 (Proxy)\")\n",
        "target_device = hub.Device(\"QCS8550 (Proxy)\")\n",
        "# Check if the device is available before submitting the job\n",
        "print(f\"4. Checking for device availability: {target_device.name}...\")\n",
        "available_devices = hub.get_devices()\n",
        "if target_device.name not in [d.name for d in available_devices]:\n",
        "    print(f\"ERROR: The device '{target_device.name}' is not currently available.\")\n",
        "    print(\"Please check the Qualcomm AI Hub website for device status and try again later.\")\n",
        "    compiled_model = None\n",
        "else:\n",
        "    print(f\"Device '{target_device.name}' is available. Submitting compilation job...\")\n",
        "    # Submit a compilation job to the Qualcomm AI Hub using the ONNX model file.\n",
        "    if onnx_model_path:\n",
        "        try:\n",
        "            # We are now targeting the QNN runtime for NPU execution.\n",
        "            compile_job = hub.submit_compile_job(\n",
        "                model=onnx_model_path,\n",
        "                name=f\"{model_name.split('/')[-1]}_qcs8550_npu\", # Updated name\n",
        "                device=target_device,\n",
        "                # The input specs must specify the correct integer type.\n",
        "                # Since we exported the ONNX model as int32, we must specify int32 here.\n",
        "                input_specs={\"input_ids\": (input_shape, \"int32\")},\n",
        "                options=\"--truncate_64bit_io\"\n",
        "            )\n",
        "\n",
        "            print(\"Compilation job submitted.\")\n",
        "            try:\n",
        "                print(f\"Job ID: {compile_job.id}\")\n",
        "            except AttributeError:\n",
        "                print(\"Could not retrieve job ID from the CompileJob object.\")\n",
        "                print(f\"You can check the job status and details on the AI Hub website using the URL: {compile_job.url}\")\n",
        "\n",
        "            # Wait for the job to complete. This is a blocking call and will\n",
        "            # raise an exception if the job fails.\n",
        "            print(\"Waiting for compilation to complete...\")\n",
        "            compile_job.wait()\n",
        "\n",
        "            # If wait() completes successfully, the model is ready.\n",
        "            print(\"Compilation job completed successfully!\")\n",
        "            compiled_model = compile_job.get_target_model()\n",
        "\n",
        "            # Handle the case where the Model object might not have an 'id' attribute.\n",
        "            try:\n",
        "                print(f\"Compiled model is ready. Model ID: {compiled_model.id}\")\n",
        "            except AttributeError:\n",
        "                print(\"Compiled model is ready, but could not retrieve the Model ID from the Model object.\")\n",
        "                print(\"Please check the Qualcomm AI Hub website to find the model and its ID.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during compilation: {e}\")\n",
        "            compiled_model = None\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Run Inference on the NPU using the Compiled Model\n",
        "# ------------------------------------------------------------------------------\n",
        "# Now we can submit an inference job to generate a response.\n",
        "if compiled_model:\n",
        "    print(\"5. Submitting inference job...\")\n",
        "    try:\n",
        "        # Define the input prompt.\n",
        "        prompt_text = \"What is the capital of Japan?\"\n",
        "        print(f\"Input prompt: '{prompt_text}'\")\n",
        "\n",
        "        # The inference job expects a dictionary of inputs. We need to tokenize\n",
        "        # the prompt and convert it to a NumPy array.\n",
        "        input_tokens = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
        "        # We explicitly cast the NumPy array to int32 to match the compiled model's\n",
        "        # expected data type.\n",
        "        input_data = {\"input_ids\": [input_tokens.cpu().numpy().astype(np.int32)]}\n",
        "\n",
        "        # Submit the inference job with the compiled model and input data.\n",
        "        inference_job = hub.submit_inference_job(\n",
        "            model=compiled_model,\n",
        "            device=target_device,\n",
        "            inputs=input_data\n",
        "        )\n",
        "\n",
        "        print(\"Inference job submitted.\")\n",
        "        try:\n",
        "            print(f\"Job ID: {inference_job.id}\")\n",
        "        except AttributeError:\n",
        "            print(\"Could not retrieve job ID from the InferenceJob object.\")\n",
        "            print(f\"You can check the job status and details on the AI Hub website using the URL: {inference_job.url}\")\n",
        "\n",
        "        # Wait for the inference job to complete.\n",
        "        print(\"Waiting for inference to complete...\")\n",
        "        inference_job.wait()\n",
        "\n",
        "        # If wait() completes successfully, the output is ready.\n",
        "        print(\"Inference job completed successfully!\")\n",
        "\n",
        "        # Get the output data, which will be a dictionary of numpy arrays.\n",
        "        # This has been corrected from `get_outputs()` to `get_output_data()`\n",
        "        output_data = inference_job.get_output_data()\n",
        "\n",
        "        # We can then process the output logits to get the generated text.\n",
        "        # This part requires a custom generation loop, as the AI Hub returns\n",
        "        # a single step of output. For a complete LLM inference, you would\n",
        "        # loop this process. Here, we'll just show the raw output as an example.\n",
        "        if 'logits' in output_data:\n",
        "            logits = output_data['logits']\n",
        "            # Here you would typically process the logits to generate the next token.\n",
        "            # For demonstration, we'll just print a confirmation.\n",
        "            print(f\"Received logits with shape: {logits.shape}\")\n",
        "            print(\"The model has successfully processed the input on the NPU.\")\n",
        "            print(\"To generate a full response, you would need to implement a token generation loop.\")\n",
        "        else:\n",
        "            print(\"Output data does not contain 'logits'.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during inference: {e}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jhlq4JFF2EWh",
        "outputId": "74a5e6af-6bcf-4467-a41d-9e0cbf0c6dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Downloading and preparing model: HuggingFaceTB/SmolLM-135M-Instruct...\n",
            "Exporting model to ONNX with a dummy input of shape: torch.Size([1, 7])...\n",
            "Model successfully exported to SmolLM-135M-Instruct.onnx\n",
            "================================================================================\n",
            "4. Checking for device availability: QCS8550 (Proxy)...\n",
            "Device 'QCS8550 (Proxy)' is available. Submitting compilation job...\n",
            "Uploading SmolLM-135M-Instruct.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 622M/622M [00:05<00:00, 110MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scheduled compile job (jp02dk725) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/jp02dk725/\n",
            "\n",
            "Compilation job submitted.\n",
            "Could not retrieve job ID from the CompileJob object.\n",
            "You can check the job status and details on the AI Hub website using the URL: https://app.aihub.qualcomm.com/jobs/jp02dk725/\n",
            "Waiting for compilation to complete...\n",
            "Waiting for compile job (jp02dk725) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ✅ SUCCESS                          \u0007\n",
            "Compilation job completed successfully!\n",
            "Compiled model is ready, but could not retrieve the Model ID from the Model object.\n",
            "Please check the Qualcomm AI Hub website to find the model and its ID.\n",
            "================================================================================\n",
            "5. Submitting inference job...\n",
            "Input prompt: 'What is the capital of Japan?'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Uploading dataset: 17.7kB [00:00, 113kB/s]                    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scheduled inference job (jp8m68vz5) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/jp8m68vz5/\n",
            "\n",
            "Inference job submitted.\n",
            "Could not retrieve job ID from the InferenceJob object.\n",
            "You can check the job status and details on the AI Hub website using the URL: https://app.aihub.qualcomm.com/jobs/jp8m68vz5/\n",
            "Waiting for inference to complete...\n",
            "Waiting for inference job (jp8m68vz5) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ✅ SUCCESS                          \u0007\n",
            "Inference job completed successfully!\n",
            "An error occurred during inference: 'InferenceJob' object has no attribute 'get_output_data'\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Download and Prepare the SmolLM Model\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "\n",
        "# We'll use the SmolLM-135M-Instruct model from Hugging Face as an example.\n",
        "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
        "print(f\"3. Downloading and preparing model: {model_name}...\")\n",
        "\n",
        "# Load the tokenizer and the model from Hugging Face.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode. This is important for conversion.\n",
        "model.eval()\n",
        "\n",
        "# To handle the dynamic nature of the past_key_values cache, we will\n",
        "# create a simple wrapper class for the model's forward pass.\n",
        "# This ensures the output is a simple tuple of tensors, which ONNX can handle.\n",
        "class SmolLMWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = model.config\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # We need to get the output from the model's forward pass.\n",
        "        # We'll set return_dict to True to easily access the outputs.\n",
        "        outputs = self.model(input_ids, return_dict=True)\n",
        "\n",
        "        # The outputs contain a DynamicCache object, which the ONNX exporter\n",
        "        # cannot handle. We need to convert it into a flat list of tensors.\n",
        "        logits = outputs.logits\n",
        "        past_key_values = outputs.past_key_values\n",
        "\n",
        "        # Flatten the tuple of tuples of tensors into a single tuple of tensors.\n",
        "        flattened_past_key_values = []\n",
        "        for layer_key_value in past_key_values:\n",
        "            flattened_past_key_values.extend(layer_key_value)\n",
        "\n",
        "        # The ONNX exporter requires the output to be a tuple of tensors.\n",
        "        return (logits,) + tuple(flattened_past_key_values)\n",
        "\n",
        "# Instantiate the wrapper and get the output names\n",
        "model_wrapper = SmolLMWrapper(model)\n",
        "num_layers = model.config.num_hidden_layers\n",
        "output_names = ['logits'] + [f'past_key_values_{i}' for i in range(num_layers * 2)]\n",
        "\n",
        "# Define dummy input\n",
        "onnx_model_path = \"SmolLM-135M-Instruct.onnx\"\n",
        "# We explicitly cast the dummy input to int32 to create an ONNX model\n",
        "# that natively expects int32. This avoids the data type mismatch during compilation.\n",
        "dummy_input = tokenizer(\"What is the capital of France?\", return_tensors=\"pt\").input_ids.to(torch.int32)\n",
        "input_shape = dummy_input.shape\n",
        "\n",
        "print(f\"Exporting model to ONNX with a dummy input of shape: {input_shape}...\")\n",
        "\n",
        "try:\n",
        "    # Use the wrapper model for export.\n",
        "    torch.onnx.export(\n",
        "        model_wrapper,\n",
        "        dummy_input,\n",
        "        onnx_model_path,\n",
        "        opset_version=14,  # Choose a compatible ONNX opset version\n",
        "        input_names=['input_ids'],\n",
        "        output_names=output_names,\n",
        "        dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'}},\n",
        "    )\n",
        "    print(f\"Model successfully exported to {onnx_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during ONNX export: {e}\")\n",
        "    onnx_model_path = None\n",
        "print(\"=\"*80)\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Compile the Model for QCS8550 NPU\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Define the target device\n",
        "#target_device = hub.Device(\"QCS8250 (Proxy)\")\n",
        "target_device = hub.Device(\"QCS8550 (Proxy)\")\n",
        "# Check if the device is available before submitting the job\n",
        "print(f\"4. Checking for device availability: {target_device.name}...\")\n",
        "available_devices = hub.get_devices()\n",
        "if target_device.name not in [d.name for d in available_devices]:\n",
        "    print(f\"ERROR: The device '{target_device.name}' is not currently available.\")\n",
        "    print(\"Please check the Qualcomm AI Hub website for device status and try again later.\")\n",
        "    compiled_model = None\n",
        "else:\n",
        "    print(f\"Device '{target_device.name}' is available. Submitting compilation job...\")\n",
        "    # Submit a compilation job to the Qualcomm AI Hub using the ONNX model file.\n",
        "    if onnx_model_path:\n",
        "        try:\n",
        "            # We are now targeting the QNN runtime for NPU execution.\n",
        "            compile_job = hub.submit_compile_job(\n",
        "                model=onnx_model_path,\n",
        "                name=f\"{model_name.split('/')[-1]}_qcs8550_npu\", # Updated name\n",
        "                device=target_device,\n",
        "                # The input specs must now specify int32 to match the ONNX model,\n",
        "                # which was created with int32 tensors.\n",
        "                input_specs={\"input_ids\": (input_shape, \"int32\")},\n",
        "                options=\"--truncate_64bit_io\"\n",
        "            )\n",
        "\n",
        "            print(\"Compilation job submitted.\")\n",
        "            try:\n",
        "                print(f\"Job ID: {compile_job.id}\")\n",
        "            except AttributeError:\n",
        "                print(\"Could not retrieve job ID from the CompileJob object.\")\n",
        "                print(f\"You can check the job status and details on the AI Hub website using the URL: {compile_job.url}\")\n",
        "\n",
        "            # Wait for the job to complete. This is a blocking call and will\n",
        "            # raise an exception if the job fails.\n",
        "            print(\"Waiting for compilation to complete...\")\n",
        "            compile_job.wait()\n",
        "\n",
        "            # If wait() completes successfully, the model is ready.\n",
        "            print(\"Compilation job completed successfully!\")\n",
        "            compiled_model = compile_job.get_target_model()\n",
        "\n",
        "            # Handle the case where the Model object might not have an 'id' attribute.\n",
        "            try:\n",
        "                print(f\"Compiled model is ready. Model ID: {compiled_model.id}\")\n",
        "            except AttributeError:\n",
        "                print(\"Compiled model is ready, but could not retrieve the Model ID from the Model object.\")\n",
        "                print(\"Please check the Qualcomm AI Hub website to find the model and its ID.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during compilation: {e}\")\n",
        "            compiled_model = None\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Run Inference on the NPU using the Compiled Model\n",
        "# ------------------------------------------------------------------------------\n",
        "# Now we can submit an inference job to generate a response.\n",
        "if compiled_model:\n",
        "    print(\"5. Submitting inference job...\")\n",
        "    try:\n",
        "        # Define the input prompt.\n",
        "        prompt_text = \"What is the capital of Japan?\"\n",
        "        print(f\"Input prompt: '{prompt_text}'\")\n",
        "\n",
        "        # The inference job expects a dictionary of inputs. We need to tokenize\n",
        "        # the prompt and convert it to a NumPy array.\n",
        "        input_tokens = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
        "        # We explicitly cast the NumPy array to int32 to match the compiled model's\n",
        "        # expected data type, which is now int32.\n",
        "        input_data = {\"input_ids\": [input_tokens.cpu().numpy().astype(np.int32)]}\n",
        "\n",
        "        # Submit the inference job with the compiled model and input data.\n",
        "        inference_job = hub.submit_inference_job(\n",
        "            model=compiled_model,\n",
        "            device=target_device,\n",
        "            inputs=input_data\n",
        "        )\n",
        "\n",
        "        print(\"Inference job submitted.\")\n",
        "        try:\n",
        "            print(f\"Job ID: {inference_job.id}\")\n",
        "        except AttributeError:\n",
        "            print(\"Could not retrieve job ID from the InferenceJob object.\")\n",
        "            print(f\"You can check the job status and details on the AI Hub website using the URL: {inference_job.url}\")\n",
        "\n",
        "        # Wait for the inference job to complete.\n",
        "        print(\"Waiting for inference to complete...\")\n",
        "        inference_job.wait()\n",
        "\n",
        "        # If wait() completes successfully, the output is ready.\n",
        "        print(\"Inference job completed successfully!\")\n",
        "\n",
        "        # Get the output data, which will be a dictionary of numpy arrays.\n",
        "        # We've corrected the method name to download_output_data() and added a check.\n",
        "        output_data = inference_job.download_output_data()\n",
        "\n",
        "        # We can then process the output logits to get the generated text.\n",
        "        # This part requires a custom generation loop, as the AI Hub returns\n",
        "        # a single step of output. For a complete LLM inference, you would\n",
        "        # loop this process. Here, we'll just show the raw output as an example.\n",
        "        if 'logits' in output_data:\n",
        "            logits = output_data['logits']\n",
        "            # Here you would typically process the logits to generate the next token.\n",
        "            # For demonstration, we'll just print a confirmation.\n",
        "            print(f\"Received logits with shape: {logits.shape}\")\n",
        "            print(\"The model has successfully processed the input on the NPU.\")\n",
        "            print(\"To generate a full response, you would need to implement a token generation loop.\")\n",
        "        else:\n",
        "            print(\"Output data does not contain 'logits'. This may indicate a problem with the model's output configuration.\")\n",
        "            print(\"Here are all the keys found in the output data for debugging:\")\n",
        "            print(output_data.keys())\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during inference: {e}\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u8EAxd-5Mt_",
        "outputId": "7a032b94-2103-4a3e-879f-1389b663c4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Downloading and preparing model: HuggingFaceTB/SmolLM-135M-Instruct...\n",
            "Exporting model to ONNX with a dummy input of shape: torch.Size([1, 7])...\n",
            "Model successfully exported to SmolLM-135M-Instruct.onnx\n",
            "================================================================================\n",
            "4. Checking for device availability: QCS8550 (Proxy)...\n",
            "Device 'QCS8550 (Proxy)' is available. Submitting compilation job...\n",
            "Uploading SmolLM-135M-Instruct.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 622M/622M [00:06<00:00, 104MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scheduled compile job (jgonomq4p) successfully. To see the status and results:\n",
            "    https://app.aihub.qualcomm.com/jobs/jgonomq4p/\n",
            "\n",
            "Compilation job submitted.\n",
            "Could not retrieve job ID from the CompileJob object.\n",
            "You can check the job status and details on the AI Hub website using the URL: https://app.aihub.qualcomm.com/jobs/jgonomq4p/\n",
            "Waiting for compilation to complete...\n",
            "Waiting for compile job (jgonomq4p) completion. Type Ctrl+C to stop waiting at any time.\n"
          ]
        }
      ]
    }
  ]
}